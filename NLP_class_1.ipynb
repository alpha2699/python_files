{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b66e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'I had such... high hopes for this dress!!!'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07ac6154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I had such high hopes for this dress'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remoe '!' and '.' characters using replace() method\n",
    "\n",
    "text1.replace('.','').replace('!','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa609cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84588c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = 'I had such... high hopes for this dress!!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81fb4752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I had such>>> high hopes for this dress>>>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'[.!]','>',text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77b594b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = 'I had such... high hopes for this dress!!!https://www.w3schools.com/html/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "852af40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_url = re.findall('http.*',text3) # find url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f6bcd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.w3schools.com/html/']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21e83155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.w3schools.com/html/']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_extract = \"https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)$\"\n",
    "re.findall(url_extract,text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0afae54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I had such... high hopes for this dress!!!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(url_extract,'',text3) #replace url with ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59d1cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e372afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b713d195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a666b81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "     ---------------------------------------- 0.0/289.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/289.9 kB ? eta -:--:--\n",
      "     ----- --------------------------------- 41.0/289.9 kB 1.9 MB/s eta 0:00:01\n",
      "     ------------ -------------------------- 92.2/289.9 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------- --------------- 174.1/289.9 kB 1.2 MB/s eta 0:00:01\n",
      "     ----------------------------- -------- 225.3/289.9 kB 1.1 MB/s eta 0:00:01\n",
      "     --------------------------------- ---- 256.0/289.9 kB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 289.9/289.9 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "  Downloading pyahocorasick-2.0.0-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "50c92eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "352980f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "stop = nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4c12f5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: I'll be there within 5 min. Shouldn't you be there too?\n",
      "          I'd love to see u there my dear. It's awesome to meet new friends.\n",
      "          We've been waiting for this day for so long.\n",
      "Expanded_text: I will be there within 5 min. Should not you be there too? I would love to see you there my dear. It is awesome to meet new friends. We have been waiting for this day for so long.\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "# contracted text\n",
    "text = '''I'll be there within 5 min. Shouldn't you be there too?\n",
    "          I'd love to see u there my dear. It's awesome to meet new friends.\n",
    "          We've been waiting for this day for so long.'''\n",
    " \n",
    "# creating an empty list\n",
    "expanded_words = []   \n",
    "for word in text.split():\n",
    "  # using contractions.fix to expand the shortened words\n",
    "  expanded_words.append(contractions.fix(word))  \n",
    "   \n",
    "expanded_text = ' '.join(expanded_words)\n",
    "print('Original text: ' + text)\n",
    "print('Expanded_text: ' + expanded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9b6aba7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "39247930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Index: 26\n",
      "End Index: 33\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "s = 'GeeksforGeeks: A computer science portal for geeks'\n",
    " \n",
    "match = re.search(r'science', s)\n",
    " \n",
    "print('Start Index:', match.start())\n",
    "print('End Index:', match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c8600b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I within 5 min. Should too? I would love see dear. It awesome meet new friends. We waiting day long.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Removing stopwords\n",
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "text = \"I will be there within 5 min. Should not you be there too? I would love to see you there my dear. It is awesome to meet new friends. We have been waiting for this day for so long.\"\n",
    "text = \" \".join([word for word in text.split() if word not in (stop)])\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600743c",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5961f836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump = jump\n",
      "jumped = jump\n",
      "jumps = jump\n",
      "caring = care\n",
      "swimming = swim\n",
      "women = women\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "words = [\"jump\", \"jumped\", \"jumps\", \"caring\",\"swimming\",\"women\"]\n",
    "stemmer = PorterStemmer()\n",
    "for word in words:\n",
    "\n",
    "    print(word + \" = \" + stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6300c837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "54a207c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump = jump\n",
      "jumped = jumped\n",
      "jumps = jump\n",
      "caring = caring\n",
      "fishes = fish\n",
      "women = woman\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "words = [\"jump\", \"jumped\", \"jumps\", \"caring\",\"fishes\", \"women\"]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for word in words:\n",
    "\n",
    "    print(word + \" = \" + lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c980ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
